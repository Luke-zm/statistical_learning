{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6622ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # for easily manipulating databases\n",
    "import numpy as np  # for numerical computations\n",
    "import matplotlib.pyplot as plt  # for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fda389",
   "metadata": {},
   "source": [
    "# Load the dataset and explore\n",
    "\n",
    "The diabetes dataset contains 11 measurements from a cohort of N = 442 patients with diabetes, and we are most interested in the column labeled “Y”, which is some clinical measure of disease progression taken one year after the rest of the measurements are taken.\n",
    "\n",
    "So we will want to predict Y from the 10 other variables, which are the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./diabetes.csv')\n",
    "\n",
    "print(df.columns) # to see the variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d99eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BMI'].hist()  # to plot a histogram of the BMI variable\n",
    "df.hist() # will give you a histogram of every variable\n",
    "# plt.show() # If you are using IPython instead of Jupyter, use this to see the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dc1713",
   "metadata": {},
   "source": [
    "### Question1:\n",
    "Which variables are continuous and which are not? \n",
    "\n",
    "\n",
    "### Question 2:\n",
    "How do we handle the variables that are not continuous in the regression model? \n",
    "\n",
    "\n",
    "### Question 3:\n",
    "'Y' is the target variable. Extract it from the dataframe and create a separate matrix of predictors where any discrete variables are encoded appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0604a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a23e3894",
   "metadata": {},
   "source": [
    "# Ordinary linear regression with statsmodels\n",
    "\n",
    "statsmodels is a popular statistic module in Python. We will use it to fit ordinary least squares on the dataset. Run the following code to fit linear regression and print the results. Note that we do not need to re-scale the predictors for least squares (though you can if you want to, as it often helps interpretation.) \n",
    "\n",
    "This code assumes you correctly prepared a the data 'Y' and 'X' as instructed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm  # load statsmodels\n",
    "\n",
    "X_ = sm.add_constant(X)  # this is how you add a constant term (for the intercept)\n",
    "est = sm.OLS(Y, X_).fit()  # fits the least squares estimate\n",
    "print(est.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8918108",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "Identify the coefficient estimates. Which predictors are we quite sure are important for the prediction? (Also, do you see the confidence intervals? What number represents the percentiles of a Gaussian distribution like you learned in lecture?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2ca16",
   "metadata": {},
   "source": [
    "# Shrinkage with scikit-learn\n",
    "\n",
    "In the statsmodels print out, we see it warn us that there is likely strong multicollinearity in the data. We therefore know that we should explore using shrinkage methods.\n",
    "\n",
    "### Question 5:\n",
    "\n",
    "Use scikit-learn to compare linear regression, ridge regression, and Lasso on this dataset. You should be using a method like cross validation to compute the average squared prediction error over multiple test sets. (If you do not want to code up K-fold cross validation by hand, instead consider using test sets that are just random subsets of the dataset.)\n",
    "\n",
    "Consider how you should choose the the penalty parameter \\lambda (it would be best to use a validation set approach, where you select \\lambda using a method like cross validation using the training data only).\n",
    "\n",
    "Note that cross validation is being used in two separate ways in this exercise: To select the penalty parameter \\lambda, and to compare predictive performance of the models.\n",
    "\n",
    "Remember that we must standardize the predictors before using ridge regression and Lasso. Below the key functions are provided for you. You should certainly read their documentation to learn more about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "\n",
    "\"\"\"\n",
    "lr = LinearRegression(fit_intercept=True).fit(X_train, Y_train)  # Linear regression with an intercept. Do NOT use X_ from statsmodels.\n",
    "Y_pred = lr.predict(X_test)  # prediction on a test set\n",
    "\n",
    "ridge = RidgeCV(alphas=np.linspace(0.001, 100.0, 100), fit_intercept=True, cv=10).fit(X_train, Y_train)  # Ridge regression with an intercept. Selects the penalty from among 0.1, 1.0, and 10 using 5-fold cross validation.\n",
    "Y_pred = ridge.predict(X_test)  # prediction on a test set\n",
    "\n",
    "lasso = LassoCV(alphas=np.linspace(0.001, 100.0, 100), fit_intercept=True, cv=10).fit(X_train, Y_train)  # Lasso with an intercept. Selects the penalty from among 100 choices linearly space in (0.001, 10) using 10-fold cross validation.\n",
    "Y_pred = lasso.predict(X_test)  # prediction on a test set\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b5327",
   "metadata": {},
   "source": [
    "### Question 6:\n",
    "Explore the weights for each of the models by visualizing them (use the documentation to figure out where the coefficients are stored). Is Lasso performing feature selection? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be418c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c071a9dc",
   "metadata": {},
   "source": [
    "### Question 7:\n",
    "Take a look at some penalty parameters that are selected by the RidgeCV and LassoCV methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61e7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01829903",
   "metadata": {},
   "source": [
    "### Question 8:\n",
    "Many people would just choose the model with the best average metric over the test sets. (Which would you choose in that case?)\n",
    "\n",
    "But I would like you to consider the *noise* over the test sets. Plot boxplots (look at the plt.boxplot() method) to compare the *populations* of test errors across the methods. Does any method look like it consistently outperforms the others?\n",
    "\n",
    "Consider running a statistical (hypothesis) test that two of these sets of scores significantly differ. (Use the scipy.stats.ttest_rel() method to run a paired t-test, which tests whether the means of two paired samples differs significantly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76435cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
