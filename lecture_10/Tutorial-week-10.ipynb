{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f055cc4-b1bc-4e89-964a-c393e7bc120b",
   "metadata": {},
   "source": [
    "# Tutorial Week 10 questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7bf15-fef1-4839-b5f1-b06f34323ae6",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8d39d-e208-4ae4-a4b2-823f3efc0c12",
   "metadata": {},
   "source": [
    "### This question is based on question 2 from Chapter 5 of your textbook, p. 224.  It derives the probability that a given observation in a data set is part of a bootstrap sample.  This is relevant to the material about OOB error estimation in the week 9 lecture.  I mentioned in lectures that there is only a tiny probability that a training observation is not out-of-bag for any bootstrap sample in bagging if the number of bootstrap samples B is moderately large.  The question will help you understand this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915445f-ea44-4575-a20c-08a77da5eee5",
   "metadata": {},
   "source": [
    "### Suppose that we obtain a bootstrap sample of n observations.\n",
    "- ### What is the probability that the first bootstrap observation is not the jth observation from the original sample?\n",
    "- ### What is the probability that the second bootstrap observation is not the jth observation from the original sample?\n",
    "- ### Argue that the probability that the jth obseravation is not in the bootstrap sample is (1-1/n)^n.\n",
    "- ### Recall from your basic mathematics that $$\\lim_{n\\rightarrow\\infty} \\left(1+\\frac{x}{n}\\right)^n = \\exp(x),$$ and deduce that for a large sample the probability that the jth observation is not in the bootstrap sample is $\\exp(-1)$ (approximately 1/3).\n",
    "- ### Suppose that we are doing bagging with B=100.  For a large sample, what is the probability that the jth sample is not out of bag for any of the bootstrap samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180a31f-8dd4-41b1-ab01-e9bdf665abf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0d460c3-53e8-47bf-8f02-0ca60c3f9c8c",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c84a8-ee6b-4117-8694-35580211a263",
   "metadata": {},
   "source": [
    "### Consider a two-dimensional feature vector and a polynomial kernel with degree 2, $$K(x,z)=(1+x_1z_1+x_2z_2)^2,$$ where $x=(x_1,x_2)$ and $z=(z_1,z_2)$ are feature vectors.  Write $K(x,z)$ in the form $$K(x,z)=\\phi(x)^T \\phi(x),$$ for an expanded six-dimensional feature vector $\\phi(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8acf0-f8fb-4b54-bb46-0005beb40343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79eb3a02-80c4-4df3-a61b-cf1ae6dc2962",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52368e4-1d4a-48a6-9d77-43180b275075",
   "metadata": {},
   "source": [
    "### This question is question 4 from Chapter 9 of your textbook.\n",
    "\n",
    "### Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation between the two classes.  Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data.  Which technique performs best on the test data?  Make plots and report training and test error rates in order to back up your assertions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c798ff-da9b-4a37-b3f6-e60382fea7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe07aca-071a-4c01-9026-fa866eebed68",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c761ed-4250-4a08-9e2f-de3635256344",
   "metadata": {},
   "source": [
    "### In the example you constructed in question 3, scale one of the features by multiplying by 100.  Refit one of the SVM classifiers you fit in question 3.  Is feature scaling important for an SVM classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3830cbb-e9d0-4d2a-9193-f3a11ea30cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
